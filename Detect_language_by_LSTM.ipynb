{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Detect_language_by_LSTM.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOZjwdWQWYtuoqGb1wBkDdq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TranQuocViet236/Somethings_on_Colab/blob/main/Detect_language_by_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "on8Cl4i9hrFw"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "#Import Keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "import re\n",
        "from distutils.version import LooseVersion\n",
        "import warnings\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "import numpy as np\n",
        "import pickle\n",
        "import sys\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1v0aZcXYv56T"
      },
      "source": [
        "Remove all of special characters, \n",
        "converting to lower case"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXjRTIblth9k"
      },
      "source": [
        "\n",
        "#Hyperparameters:\n",
        "max_sentence_length = 200\n",
        "embedding_vector_length = 300\n",
        "dropout = 0.5\n",
        "sentence = \"   I love) you?  \"\n",
        "def process_sentence(sentence):\n",
        "  new_sentence = sentence.lower()\n",
        "  return re.sub(r'[\\\\\\\\/:*«`\\'?¿\";!<>,.|]]','', new_sentence.strip())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUSaa-vTv-pY"
      },
      "source": [
        "Create a table to look up Vocab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gH7SBmlStia1"
      },
      "source": [
        "def create_lookup_table(text):\n",
        "  #parameter text will be devided into words\n",
        "  #return: (vocab_to_int, int_to_vocab)\n",
        "\n",
        "  vocab = set(text)\n",
        "\n",
        "  vocab_to_int = {word: i for i, word in enumerate(vocab)}\n",
        "  int_to_vocab = {v: k for k,v in vocab_to_int.items()}\n",
        "  return (vocab_to_int, int_to_vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZL3QDvzlPziz"
      },
      "source": [
        "# print(create_lookup_table('Anh yeu em nhieu lam'))\n",
        "# #Convert text into number\n",
        "def convert_to_int(data, data_int):\n",
        "\n",
        "  all_items = []\n",
        "  for sentence in data:\n",
        "    all_items.append([data_int[word] if word in data_int else data_int[\"unk\"] for word in sentence.split()])\n",
        "  return all_items\n",
        "\n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tk4u0XQ5R0nh"
      },
      "source": [
        "#Load data from file\n",
        "def load_data(file_path):\n",
        "  data = pd.read_csv(file_path, names = ['sentence', 'language'], header = None)\n",
        "  print(data.describe())\n",
        "  return data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_9HzDIDVJrS"
      },
      "source": [
        "#Building model\n",
        "def get_model():\n",
        "  model = Sequential()\n",
        "\n",
        "  model.add(Embedding(len(vocab_to_int), embedding_vector_length, input_length=max_sentence_length))\n",
        "  model.add(LSTM(256, return_sequences = True, dropout=dropout, recurrent_dropout=dropout ))\n",
        "  model.add(LSTM(256, dropout=dropout, recurrent_dropout= dropout))\n",
        "  model.add(Dense(len(languages), activation='softmax'))\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['acc'])\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGVTY-zrW_OV"
      },
      "source": [
        "#Convert an input sentence to integer vector and predict base on model\n",
        "def predict_sentence(model, sentence, vocab_to_int, idx_to_language):\n",
        "  #Clean the sentence\n",
        "  sentence = process_sentence(sentence)\n",
        "  sen_encode = convert_to_int(sentence, vocab_to_int)\n",
        "  # enc = OneHotEncoder()\n",
        "  # X = enc.fit_transform(convert_to_int(sentence, languages_to_int)).toarray()\n",
        "\n",
        "  #Transform and pad it before using the model to predict\n",
        "  # X = np.array(convert_to_int([sentence], vocab_to_int))\n",
        "  X = sequence.pad_senquences(sen_encode, maxlen= max_sentence_length)\n",
        "  #pad_senquence is used to convert a matrix into another matrix has the same length and is max_sentence_length\n",
        "\n",
        "  prediction = model.predict(X)\n",
        "\n",
        "  #Get the highest prediction\n",
        "  lang_index = np.argmax(prediction)\n",
        "  print(prediction[0][lang_index])\n",
        "\n",
        "  # If the probality < 0.3, cannot determine the language\n",
        "\n",
        "  if prediction[0][lang_index] < 0.3:\n",
        "    return 'Unknown'\n",
        "  else:\n",
        "    return idx_to_language[lang_index]\n",
        "    \n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnTh2bfubnxG"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sRtuFP-cJD9"
      },
      "source": [
        "#load and shuffle dataframe\n",
        "filepath = '/content/gdrive/MyDrive/ML/LSTM/language_data.csv'\n",
        "data = load_data(filepath)\n",
        "\n",
        "sss = StratifiedShuffleSplit(test_size = 0.2, random_state=0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDPVMB0hafnd"
      },
      "source": [
        "#Process sentences:\n",
        "X = data['sentence'].apply(process_sentence)\n",
        "y = data['language']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeqgWehinczj"
      },
      "source": [
        "#Divide data into sentences\n",
        "elements = (' '.join([sentence for sentence in X])).split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZOYTBV3A_Ec"
      },
      "source": [
        "X_train, X_test, y_train, y_test = None, None, None, None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9fSWLTaBMuw"
      },
      "source": [
        "print(sss.split(X,y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxK2f4idBRFc"
      },
      "source": [
        "for train_index, test_index in sss.split(X,y):\n",
        "  X_train, X_test = X[train_index], X[test_index]\n",
        "  y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "languages = set(y)\n",
        "elements.append('unk')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tf0y8zebqizk"
      },
      "source": [
        "languages\n",
        "elements"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buh3JNzLrDgG"
      },
      "source": [
        "(vocab_to_int, int_to_vocab) = create_lookup_table(elements)\n",
        "(languages_to_int, int_to_languages) = create_lookup_table(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZiH4roQstuG"
      },
      "source": [
        "print(vocab_to_int)\n",
        "print(int_to_vocab)\n",
        "print(languages_to_int)\n",
        "print(int_to_languages)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eU2qS-6qsxHA"
      },
      "source": [
        "X_train_encode = convert_to_int(X_train, vocab_to_int)\n",
        "X_test_encode = convert_to_int(X_test, vocab_to_int)\n",
        "\n",
        "y_data = convert_to_int(y_test, languages_to_int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzSJVuSttwZB"
      },
      "source": [
        "enc = OneHotEncoder()\n",
        "y_train_encode = enc.fit_transform(convert_to_int(y_train, languages_to_int)).toarray()\n",
        "y_test_encode = enc.fit_transform(convert_to_int(y_test, languages_to_int)).toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1Zq0zxHtyya"
      },
      "source": [
        "\n",
        "X_train_pad = sequence.pad_sequences(X_train_encode,maxlen= max_sentence_length)\n",
        "X_test_pad = sequence.pad_sequences(X_test_encode, maxlen= max_sentence_length)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBEMJBuewIpp"
      },
      "source": [
        "print(X_train_pad)\n",
        "print(X_test_pad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Co9y58z9wbLR"
      },
      "source": [
        "model = get_model()\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pT5C8Tiwjrk"
      },
      "source": [
        "model.fit(X_train_pad, y_train_encode, epochs=5, batch_size=256)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GI6oC0LyqDr"
      },
      "source": [
        "# Danh gia model\n",
        "scores = model.evaluate(X_test_pad, y_test_encode, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
        "\n",
        "# Luu Model vao file\n",
        "model.save(\"model.h5\")\n",
        "print(\"Model trained and saved!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GGkhPfd3hPS"
      },
      "source": [
        "model = model.load_weights(\"model.h5\")\n",
        "print(\"Model loaded!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uVh02Eo5h_V"
      },
      "source": [
        "# predict_sentence(model, \"I love you\", vocab_to_int, languages_to_int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7wfj7Qj_2dX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULafHw305uTO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}